{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZcsjmC4CUM-",
    "outputId": "249f231c-4068-410e-9fc6-279f50831572"
   },
   "source": [
    "# TP2 - Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FiZ2D8M5CWbX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 1 : On considère les fichiers du répertoire bbcsport\n",
    "* Indiquer les points marquants l'exploration.\n",
    "* Pour chaque observation, indiquer l’opération à effectuer qui serait la plus appropriée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (737, 2) \n",
      "\n",
      "Dataframe original:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>categorie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Claxton hunting first major medal\\n\\nBritish hurdler Sarah Claxton is confident she can win her ...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O'Sullivan could run in Worlds\\n\\nSonia O'Sullivan has indicated that she would like to particip...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Greene sets sights on world title\\n\\nMaurice Greene aims to wipe out the pain of losing his Olym...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IAAF launches fight against drugs\\n\\nThe IAAF - athletics' world governing body - has met anti-d...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dibaba breaks 5,000m world record\\n\\nEthiopia's Tirunesh Dibaba set a new world record in winnin...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Isinbayeva claims new world best\\n\\nPole vaulter Yelena Isinbayeva broke her own indoor world re...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O'Sullivan commits to Dublin race\\n\\nSonia O'Sullivan will seek to regain her title at the Bupa ...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hansen 'delays return until 2006'\\n\\nBritish triple jumper Ashia Hansen has ruled out a comeback...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Off-colour Gardener storms to win\\n\\nBritain's Jason Gardener shook off an upset stomach to win ...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Collins to compete in Birmingham\\n\\nWorld and Commonwealth 100m champion Kim Collins will compet...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Radcliffe yet to answer GB call\\n\\nPaula Radcliffe has been granted extra time to decide whether...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Edwards tips Idowu for Euro gold\\n\\nWorld outdoor triple jump record holder and BBC pundit Jonat...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kenya lift Chepkemei's suspension\\n\\nKenya's athletics body has reversed a ban on marathon runne...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>McIlroy aiming for Madrid title\\n\\nNorthern Ireland man James McIlroy is confident he can win hi...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UK Athletics agrees new kit deal\\n\\nUK Athletics has agreed a new deal with adidas to supply Gre...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Verdict delay for Greek sprinters\\n\\nGreek athletics' governing body has postponed by two weeks ...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Call for Kenteris to be cleared\\n\\nKostas Kenteris' lawyer has called for the doping charges aga...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Merritt close to indoor 400m mark\\n\\nTeenager LaShawn Merritt ran the third fastest indoor 400m ...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>London hope over Chepkemei\\n\\nLondon Marathon organisers are hoping that banned athlete Susan Ch...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Edwards tips Idowu for Euro gold\\n\\nWorld outdoor triple jump record holder and BBC pundit Jonat...</td>\n",
       "      <td>athletics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  texte  \\\n",
       "0   Claxton hunting first major medal\\n\\nBritish hurdler Sarah Claxton is confident she can win her ...   \n",
       "1   O'Sullivan could run in Worlds\\n\\nSonia O'Sullivan has indicated that she would like to particip...   \n",
       "2   Greene sets sights on world title\\n\\nMaurice Greene aims to wipe out the pain of losing his Olym...   \n",
       "3   IAAF launches fight against drugs\\n\\nThe IAAF - athletics' world governing body - has met anti-d...   \n",
       "4   Dibaba breaks 5,000m world record\\n\\nEthiopia's Tirunesh Dibaba set a new world record in winnin...   \n",
       "5   Isinbayeva claims new world best\\n\\nPole vaulter Yelena Isinbayeva broke her own indoor world re...   \n",
       "6   O'Sullivan commits to Dublin race\\n\\nSonia O'Sullivan will seek to regain her title at the Bupa ...   \n",
       "7   Hansen 'delays return until 2006'\\n\\nBritish triple jumper Ashia Hansen has ruled out a comeback...   \n",
       "8   Off-colour Gardener storms to win\\n\\nBritain's Jason Gardener shook off an upset stomach to win ...   \n",
       "9   Collins to compete in Birmingham\\n\\nWorld and Commonwealth 100m champion Kim Collins will compet...   \n",
       "10  Radcliffe yet to answer GB call\\n\\nPaula Radcliffe has been granted extra time to decide whether...   \n",
       "11  Edwards tips Idowu for Euro gold\\n\\nWorld outdoor triple jump record holder and BBC pundit Jonat...   \n",
       "12  Kenya lift Chepkemei's suspension\\n\\nKenya's athletics body has reversed a ban on marathon runne...   \n",
       "13  McIlroy aiming for Madrid title\\n\\nNorthern Ireland man James McIlroy is confident he can win hi...   \n",
       "14  UK Athletics agrees new kit deal\\n\\nUK Athletics has agreed a new deal with adidas to supply Gre...   \n",
       "15  Verdict delay for Greek sprinters\\n\\nGreek athletics' governing body has postponed by two weeks ...   \n",
       "16  Call for Kenteris to be cleared\\n\\nKostas Kenteris' lawyer has called for the doping charges aga...   \n",
       "17  Merritt close to indoor 400m mark\\n\\nTeenager LaShawn Merritt ran the third fastest indoor 400m ...   \n",
       "18  London hope over Chepkemei\\n\\nLondon Marathon organisers are hoping that banned athlete Susan Ch...   \n",
       "19  Edwards tips Idowu for Euro gold\\n\\nWorld outdoor triple jump record holder and BBC pundit Jonat...   \n",
       "\n",
       "    categorie  \n",
       "0   athletics  \n",
       "1   athletics  \n",
       "2   athletics  \n",
       "3   athletics  \n",
       "4   athletics  \n",
       "5   athletics  \n",
       "6   athletics  \n",
       "7   athletics  \n",
       "8   athletics  \n",
       "9   athletics  \n",
       "10  athletics  \n",
       "11  athletics  \n",
       "12  athletics  \n",
       "13  athletics  \n",
       "14  athletics  \n",
       "15  athletics  \n",
       "16  athletics  \n",
       "17  athletics  \n",
       "18  athletics  \n",
       "19  athletics  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de doublons : 10\n",
      "categorie\n",
      "football     262\n",
      "rugby        146\n",
      "cricket      121\n",
      "tennis        99\n",
      "athletics     99\n",
      "Name: count, dtype: int64\n",
      "(727, 2)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Définir le chemin du répertoire principal\n",
    "base_dir = \"c:/tmp/bbcsport\"\n",
    "\n",
    "# Initialiser une liste pour stocker les données\n",
    "data = []\n",
    "\n",
    "# Parcourir chaque sous-répertoire\n",
    "for category in os.listdir(base_dir):\n",
    "    category_path = os.path.join(base_dir, category)\n",
    "    \n",
    "    # Vérifier si c'est bien un répertoire\n",
    "    if os.path.isdir(category_path):\n",
    "        # Lire tous les fichiers dans le sous-répertoire\n",
    "        for file_name in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, file_name)\n",
    "            \n",
    "            # Lire le contenu du fichier texte\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "                text = file.read()\n",
    "                \n",
    "            # Ajouter aux données\n",
    "            data.append((text, category))\n",
    "    \n",
    "\n",
    "# Créer le DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"texte\", \"categorie\"])\n",
    "\n",
    "#from sklearn.utils import resample\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "#df.rename(columns={\"index\": \"ID\"}, inplace=True)\n",
    "\n",
    "print('Dimensions:',df.shape, '\\n')\n",
    "\n",
    "maxCharacters = 100\n",
    "pd.set_option(\"display.max_colwidth\", maxCharacters)  # Affiche jusqu'à maxCharacters caractères\n",
    "\n",
    "# Afficher les 20 premières lignes du DataFrame avec retours à la ligne\n",
    "print('Dataframe original:')\n",
    "display(df.head(20))\n",
    "\n",
    "# Mélanger les lignes du DataFrame de façon aléatoire\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"Nombre de doublons : {df.duplicated().sum()}\")\n",
    "df = df.drop_duplicates() # Suppression des doublons\n",
    "print(df['categorie'].value_counts())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rééchantillonnage du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_size: 149\n",
      "\n",
      "Dataframe après équilibrage:\n",
      "categorie\n",
      "football     149\n",
      "rugby        146\n",
      "cricket      121\n",
      "tennis        99\n",
      "athletics     99\n",
      "Name: count, dtype: int64\n",
      "catégories: ['football' 'tennis' 'athletics' 'rugby' 'cricket']\n",
      "Nombre de lignes dupliquées : 0\n",
      "Dimensions dataset rebalancé: (614, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Taille cible : on va utiliser la taille de la catégorie majoritaire\n",
    "target_size = int(df['categorie'].value_counts().max()*4/7)\n",
    "print('target_size:', target_size)\n",
    "\n",
    "# Sous-échantillonner 'football'\n",
    "df_football_resampled = resample(df[df['categorie'] == 'football'], \n",
    "                                 replace=False,  # Sous-échantillonnage\n",
    "                                 n_samples=target_size, \n",
    "                                 random_state=42)\n",
    "\n",
    "# Conserver les autres catégories sans modification\n",
    "df_other = df[df['categorie'] != 'football']\n",
    "\n",
    "# Concaténer les deux ensembles pour obtenir le DataFrame final\n",
    "df_balanced = pd.concat([df_football_resampled, df_other])\n",
    "\n",
    "print(\"\\nDataframe après équilibrage:\")\n",
    "# Éliminer les doublons\n",
    "df = df_balanced.copy()\n",
    "df = df.drop_duplicates()\n",
    "#df.dropna(inplace=True) # supprime toutes les lignes où au moins une colonne a une valeur NaN.\n",
    "\n",
    "print(df['categorie'].value_counts())  # Vérifier le nouvel équilibre des classes\n",
    "print('catégories:',df['categorie'].unique())  # Vérifier les categoris uniques\n",
    "print(f\"Nombre de lignes dupliquées : {df.duplicated().sum()}\") # Verifier s'il y'a des doublons\n",
    "print('Dimensions dataset rebalancé:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des mots-clés par catégorie et appliquer des prédicteurs de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>categorie</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>num_short_words</th>\n",
       "      <th>num_long_words</th>\n",
       "      <th>...</th>\n",
       "      <th>england</th>\n",
       "      <th>game</th>\n",
       "      <th>new</th>\n",
       "      <th>play</th>\n",
       "      <th>said</th>\n",
       "      <th>team</th>\n",
       "      <th>time</th>\n",
       "      <th>win</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mourinho sends out warning shot\\n\\nChelsea boss Jose Mourinho believes his team's Carling Cup wi...</td>\n",
       "      <td>football</td>\n",
       "      <td>1308</td>\n",
       "      <td>247</td>\n",
       "      <td>12</td>\n",
       "      <td>123</td>\n",
       "      <td>4.283401</td>\n",
       "      <td>20.583333</td>\n",
       "      <td>122</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089253</td>\n",
       "      <td>0.407418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kenyon denies Robben Barca return\\n\\nChelsea chief executive Peter Kenyon has played down report...</td>\n",
       "      <td>football</td>\n",
       "      <td>1218</td>\n",
       "      <td>221</td>\n",
       "      <td>12</td>\n",
       "      <td>101</td>\n",
       "      <td>4.502262</td>\n",
       "      <td>18.416667</td>\n",
       "      <td>95</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Strachan turns down Pompey\\n\\nFormer Southampton manager Gordon Strachan has rejected the chance...</td>\n",
       "      <td>football</td>\n",
       "      <td>1463</td>\n",
       "      <td>241</td>\n",
       "      <td>12</td>\n",
       "      <td>91</td>\n",
       "      <td>5.058091</td>\n",
       "      <td>20.083333</td>\n",
       "      <td>82</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499980</td>\n",
       "      <td>0.582279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352086</td>\n",
       "      <td>0.535730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dundee Utd 4-1 Aberdeen\\n\\nDundee United eased into the semi-final of the Scottish Cup with an e...</td>\n",
       "      <td>football</td>\n",
       "      <td>2611</td>\n",
       "      <td>451</td>\n",
       "      <td>27</td>\n",
       "      <td>160</td>\n",
       "      <td>4.760532</td>\n",
       "      <td>16.703704</td>\n",
       "      <td>148</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.787505</td>\n",
       "      <td>0.278784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mansfield 0-1 Leyton Orient\\n\\nAn second-half goal from Andy Scott condemned Mansfield to a nint...</td>\n",
       "      <td>football</td>\n",
       "      <td>820</td>\n",
       "      <td>126</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>5.452381</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.702031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 texte  \\\n",
       "0  Mourinho sends out warning shot\\n\\nChelsea boss Jose Mourinho believes his team's Carling Cup wi...   \n",
       "1  Kenyon denies Robben Barca return\\n\\nChelsea chief executive Peter Kenyon has played down report...   \n",
       "2  Strachan turns down Pompey\\n\\nFormer Southampton manager Gordon Strachan has rejected the chance...   \n",
       "3  Dundee Utd 4-1 Aberdeen\\n\\nDundee United eased into the semi-final of the Scottish Cup with an e...   \n",
       "4  Mansfield 0-1 Leyton Orient\\n\\nAn second-half goal from Andy Scott condemned Mansfield to a nint...   \n",
       "\n",
       "  categorie  num_chars  num_words  num_sentences  num_stop_words  \\\n",
       "0  football       1308        247             12             123   \n",
       "1  football       1218        221             12             101   \n",
       "2  football       1463        241             12              91   \n",
       "3  football       2611        451             27             160   \n",
       "4  football        820        126             10              32   \n",
       "\n",
       "   avg_word_length  avg_sentence_length  num_short_words  num_long_words  ...  \\\n",
       "0         4.283401            20.583333              122              47  ...   \n",
       "1         4.502262            18.416667               95              42  ...   \n",
       "2         5.058091            20.083333               82              66  ...   \n",
       "3         4.760532            16.703704              148              93  ...   \n",
       "4         5.452381            12.600000               32              45  ...   \n",
       "\n",
       "   england      game       new  play      said      team      time       win  \\\n",
       "0      0.0  0.126744  0.000000   0.0  0.089253  0.407418  0.000000  0.899989   \n",
       "1      0.0  0.723133  0.000000   0.0  0.000000  0.000000  0.690709  0.000000   \n",
       "2      0.0  0.499980  0.582279   0.0  0.352086  0.535730  0.000000  0.000000   \n",
       "3      0.0  0.549649  0.000000   0.0  0.000000  0.000000  0.787505  0.278784   \n",
       "4      0.0  0.702031  0.000000   0.0  0.000000  0.000000  0.000000  0.712146   \n",
       "\n",
       "   world  year  \n",
       "0    0.0   0.0  \n",
       "1    0.0   0.0  \n",
       "2    0.0   0.0  \n",
       "3    0.0   0.0  \n",
       "4    0.0   0.0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614,)\n",
      "(614, 63)\n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "# Charger le modèle NLP anglais pour l'analyse linguistique (verbes, noms, etc.)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Définir les catégories sportives\n",
    "categories = ['tennis', 'athletics', 'rugby', 'cricket', 'football']\n",
    "\n",
    "# Définir les mots-clés spécifiques à chaque sport\n",
    "keywords = {\n",
    "    \"tennis\": [\"serve\", \"racket\", \"wimbledon\", \"grand slam\", \"ace\", \"match\", \"court\"],\n",
    "    \"athletics\": [\"100m\", \"hurdles\", \"relay\", \"track\", \"marathon\", \"jump\", \"sprint\"],\n",
    "    \"rugby\": [\"scrum\", \"try\", \"tackle\", \"ruck\", \"conversion\", \"union\", \"league\"],\n",
    "    \"cricket\": [\"innings\", \"bowler\", \"batsman\", \"wicket\", \"yorker\", \"over\", \"stump\"],\n",
    "    \"football\": [\"goal\", \"penalty\", \"striker\", \"midfield\", \"defender\", \"league\", \"cup\"]\n",
    "}\n",
    "\n",
    "# Fusionner tous les mots-clés\n",
    "all_keywords = list(set([word for sublist in keywords.values() for word in sublist]))\n",
    "\n",
    "def extract_features(text):\n",
    "    doc = nlp(text.lower())  # Convertir en minuscule pour l'uniformité\n",
    "\n",
    "    # Prédicteurs structurels\n",
    "    num_chars = len(text)\n",
    "    num_words = len(text.split())\n",
    "    num_sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    avg_word_length = np.mean([len(word) for word in text.split()]) if num_words > 0 else 0\n",
    "    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
    "    num_short_words = sum(1 for word in text.split() if len(word) <= 3)\n",
    "    num_long_words = sum(1 for word in text.split() if len(word) >= 7)\n",
    "\n",
    "    # Charger la liste des stopwords en anglais\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Fonction pour compter les stopwords dans un texte\n",
    "    words = text.split()  # Découper en mots\n",
    "    num_stop_words = sum(1 for word in words if word.lower() in stop_words)\n",
    "    \n",
    "    # Mots-clés spécifiques aux sports\n",
    "    keyword_counts = [text.lower().count(word) for word in all_keywords]\n",
    "\n",
    "    # Fréquences lexicales et linguistiques\n",
    "    num_unique_words = len(set(text.split()))\n",
    "    lexical_richness = num_unique_words / num_words if num_words > 0 else 0\n",
    "    num_verbs = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "    num_nouns = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
    "    num_digits = sum(1 for char in text if char.isdigit())\n",
    "    num_uppercase = sum(1 for char in text if char.isupper())\n",
    "\n",
    "    # NER: Compter les entités pertinentes\n",
    "    num_athletes = sum(1 for ent in doc.ents if ent.label_ in [\"PERSON\"])  # Compte les noms d'athlètes (ex: Usain Bolt, Roger Federer).\n",
    "    num_competitions = sum(1 for ent in doc.ents if ent.label_ in [\"EVENT\"]) # Compte les compétitions (ex: Wimbledon, Olympic Games).\n",
    "    num_locations = sum(1 for ent in doc.ents if ent.label_ in [\"GPE\", \"LOC\"]) # Compte les lieux géographiques (ex: Paris, Manchester).\n",
    "    num_proper_nouns = sum(1 for token in doc if token.pos_ == \"PROPN\") # Compte les noms propres (ex: \"Messi\", \"Ronaldo\").\n",
    "    \n",
    "    # Caractères spécifiques et ponctuation\n",
    "    num_punctuation = sum(1 for char in text if char in \".,!?\")\n",
    "    num_hashtags = text.count(\"#\")\n",
    "    num_mentions = text.count(\"@\")\n",
    "\n",
    "    return [\n",
    "        num_chars, num_words, num_sentences, num_stop_words, avg_word_length, avg_sentence_length,\n",
    "        num_short_words, num_long_words, num_unique_words, lexical_richness,\n",
    "        num_verbs, num_nouns, num_digits, num_uppercase, num_punctuation,\n",
    "        num_hashtags, num_mentions\n",
    "    ] + keyword_counts\n",
    "\n",
    "# Appliquer la fonction à la colonne 'texte'\n",
    "df_features = df['texte'].apply(lambda x: extract_features(str(x)))\n",
    "feature_names = [\n",
    "    \"num_chars\", \"num_words\", \"num_sentences\", \"num_stop_words\", \"avg_word_length\", \"avg_sentence_length\",\n",
    "    \"num_short_words\", \"num_long_words\", \"num_unique_words\", \"lexical_richness\",\n",
    "    \"num_verbs\", \"num_nouns\", \"num_digits\", \"num_uppercase\", \"num_punctuation\",\n",
    "    \"num_hashtags\", \"num_mentions\"\n",
    "] + all_keywords  # Ajouter les mots-clés aux colonnes\n",
    "\n",
    "df_features_init = pd.DataFrame(df_features.tolist(), columns=feature_names)\n",
    "\n",
    "# Ajouter TF-IDF des mots les plus discriminants\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10)  # Garder les 10 meilleurs mots\n",
    "tfidf_matrix = vectorizer.fit_transform(df['texte'])\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concaténer toutes les features\n",
    "df = df.reset_index(drop=True)\n",
    "df_features_init = df_features_init.reset_index(drop=True)\n",
    "df_features_base = pd.concat([df_features_init, df_tfidf], axis=1)\n",
    "print(df_features_base.shape)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df_features_base = df_features_base.reset_index(drop=True)\n",
    "df = pd.concat([df, df_features_base], axis=1)\n",
    "display(df.head())  # Afficher les premières lignes du DataFrame final\n",
    "print(df['categorie'].shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage de la cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 61)\n",
      "(614,)\n",
      "(614,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Définir les labels de sport (catégories)\n",
    "categories = ['tennis', 'athletics', 'rugby', 'cricket', 'football']\n",
    "le = LabelEncoder()  # Pour encoder les catégories de sport sous forme numérique\n",
    "df['category_encoded'] = le.fit_transform(df['categorie'])  # 'category' est la colonne avec les labels de sport\n",
    "\n",
    "# Séparer les features et les labels\n",
    "X = df_features_base  # Les features extraites\n",
    "y = df['category_encoded']  # Les labels encodés\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(df['categorie'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profilage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['category_encoded', 'num_chars', 'num_words', 'num_sentences', 'num_stop_words', 'avg_word_length', 'avg_sentence_length', 'num_short_words', 'num_long_words', 'num_unique_words', 'lexical_richness', 'num_verbs', 'num_nouns', 'num_digits', 'num_uppercase', 'num_punctuation', 'num_hashtags', 'num_mentions', 'scrum', 'defender', 'wicket', 'bowler', 'league', 'stump', 'ace', '100m', 'conversion', 'match', 'over', 'jump', 'relay', 'ruck', 'yorker', 'penalty', 'striker', 'wimbledon', 'track', 'serve', 'racket', 'goal', 'midfield', 'union', 'cup', 'sprint', 'marathon', 'try', 'court', 'innings', 'batsman', 'tackle', 'grand slam', 'hurdles', 'england', 'game', 'new', 'play', 'said', 'team', 'time', 'win', 'world', 'year']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38eaddfa0e048bdbb165073e758d83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                             |                                             | [  0%]   00:00 ->…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report profile_sortie.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n",
      "(614, 62)\n"
     ]
    }
   ],
   "source": [
    "import sweetviz as sv\n",
    "df_profiled = pd.concat([df['category_encoded'], df_features_base], axis=1)\n",
    "print(df_profiled.columns.tolist())  # Vérifie les noms exacts des colonnes\n",
    "#df_profiled.columns = df_profiled.columns.str.strip()  # Supprime les espaces invisibles\n",
    "#df_profiled = df_profiled.loc[:, ~df_profiled.columns.duplicated()]  # Supprime les colonnes en double\n",
    "report = sv.analyze(df_profiled)\n",
    "report.show_html(\"profile_sortie.html\")\n",
    "print(df_profiled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélection de Prédicteurs par Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "Top 20 features sélectionnées: ['england', 'wicket', 'world', 'bowler', 'avg_word_length', 'num_digits', 'striker', 'goal', 'play', 'batsman', 'sprint', 'said', 'avg_sentence_length', 'num_long_words', 'game', 'num_uppercase', 'match', 'wimbledon', 'league', 'year']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Entraîner le modèle Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Récupérer l'importance des features\n",
    "importances = rf.feature_importances_\n",
    "sorted_indices = np.argsort(importances)[::-1]  # Tri décroissant\n",
    "print(len(sorted_indices))\n",
    "\n",
    "# Sélectionner les 20 meilleures features\n",
    "feature_names = df_features_base.columns\n",
    "top_k = 20\n",
    "best_features = [feature_names[i] for i in sorted_indices[:top_k]]\n",
    "\n",
    "# Créer un nouveau DataFrame X_rfe avec ces features\n",
    "X_rfe = X[best_features]\n",
    "\n",
    "# Affichage des features sélectionnées\n",
    "print(\"Top 20 features sélectionnées:\", best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape 3: Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement du texte\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)  # Suppression des ponctuations\n",
    "    text = re.sub(\"\\d+\", \"\", text)  # Suppression des chiffres\n",
    "    return text\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"texte\"].apply(clean_text)\n",
    "\n",
    "# Séparation des données X et y\n",
    "X = df[\"cleaned_text\"]\n",
    "y = df[\"categorie\"]\n",
    "\n",
    "# Séparation train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "df_metrics = pd.DataFrame(columns=[\"Technique\", \"Modele\", \"Score F1\", \"Accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 4: Utilisation d'une technique d’extraction (par ex: TF–IDF Vectors) et choisir au moins 3 algorithmes de classification. De ce fait, on obtient au moins 3 modèles. On fera la comparaison en se basant la technique d’extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèles de classification\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=10000, solver=\"lbfgs\"),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Entraînement et évaluation\n",
    "def appliquer_model_classification(technique, train_x, test_x):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(train_x, y_train)\n",
    "        y_pred = model.predict(test_x)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results[name] = {\"F1-score\": f1, \"Accuracy\": accuracy}\n",
    "        df_metrics.loc[len(df_metrics)] = [technique, name, f1, accuracy]\n",
    "        #print(technique, ',  ', name)\n",
    "        #print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Affichage des résultats\n",
    "    print(\"\\nComparaison des scores F1 et Accuracy:\")\n",
    "    for model, scores in results.items():\n",
    "        print(f\"{model}: F1-score = {scores['F1-score']:.4f}, Accuracy = {scores['Accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words: ['the', 'to', 'a', 'in', 'and', 'of', 'for', 'on', 'he', 'was', 'i', 'but', 'is', 'with', 'his', 'it', 'at', 'that', 'have', 'be', 'said', 'has', 'will', 'we', 'as', 'from', 'not', 'after', 'by', 'had', 'their', 'an', 'are', 'they', 'who', 'been', 'first', 'out', 'this', 'england', 'when', 'over', 'against', 'game', 'were', 'two', 'win', 'all', 'there', 'm', 'last', 'up', 'world', 'would', 'one', 'if', 'its', 'you', 'play', 'new', 'players', 'time', 'before', 'back', 'team', 'just', 'my', 'also', 'can', 'second', 'three', 'she', 'which', 'her', 'off', 'match', 'only', 'him', 'now', 'cup', 'very', 'side', 'six', 'test', 'into', 'more', 'so', 'good', 'set', 'about', 'well', 'could', 'then', 'year', 'final', 'me', 'wales', 'coach', 'four', 'them']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "all_words = \" \".join(df[\"cleaned_text\"]).split()\n",
    "word_freq = Counter(all_words)\n",
    "most_common_words = [word for word, _ in word_freq.most_common(2000)]  # Top 2000 mots\n",
    "print('most common words:', most_common_words[:100]) #  Affiche les 100 premiers éléments de la liste.\n",
    "techniques = {\n",
    "    \"TfidfVectorizer\": TfidfVectorizer(),\n",
    "    \"Bag of Words\": CountVectorizer(),\n",
    "    \"Word Embeddings\": None,\n",
    "    \"NLP based features\": None,\n",
    "    \"Prédicteurs de base\": CountVectorizer(vocabulary=most_common_words),\n",
    "    \"Prédicteurs de base sélectionnés\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Technique: TfidfVectorizer\n",
      "\n",
      "Comparaison des scores F1 et Accuracy:\n",
      "Logistic Regression: F1-score = 0.9838, Accuracy = 0.9837\n",
      "Random Forest: F1-score = 0.9838, Accuracy = 0.9837\n",
      "SVM: F1-score = 0.9758, Accuracy = 0.9756\n",
      "KNN: F1-score = 0.9360, Accuracy = 0.9350\n",
      "\n",
      "\n",
      "==================================================\n",
      "Technique: Bag of Words\n",
      "\n",
      "Comparaison des scores F1 et Accuracy:\n",
      "Logistic Regression: F1-score = 0.9755, Accuracy = 0.9756\n",
      "Random Forest: F1-score = 0.9594, Accuracy = 0.9593\n",
      "SVM: F1-score = 0.8724, Accuracy = 0.8699\n",
      "KNN: F1-score = 0.6497, Accuracy = 0.6504\n",
      "\n",
      "\n",
      "\n",
      "Comparaison des scores F1 et Accuracy:\n",
      "Logistic Regression: F1-score = 0.3408, Accuracy = 0.3740\n",
      "Random Forest: F1-score = 0.5007, Accuracy = 0.5041\n",
      "SVM: F1-score = 0.1065, Accuracy = 0.2439\n",
      "KNN: F1-score = 0.3584, Accuracy = 0.3577\n",
      "\n",
      "\n",
      "==================================================\n",
      "Technique: Prédicteurs de base\n",
      "\n",
      "Comparaison des scores F1 et Accuracy:\n",
      "Logistic Regression: F1-score = 0.9757, Accuracy = 0.9756\n",
      "Random Forest: F1-score = 0.9837, Accuracy = 0.9837\n",
      "SVM: F1-score = 0.8724, Accuracy = 0.8699\n",
      "KNN: F1-score = 0.6818, Accuracy = 0.6829\n",
      "\n",
      "\n",
      "==================================================\n",
      "Technique: Prédicteurs de base sélectionnés\n",
      "Shape: (614, 20)\n",
      "\n",
      "Comparaison des scores F1 et Accuracy:\n",
      "Logistic Regression: F1-score = 0.7531, Accuracy = 0.7561\n",
      "Random Forest: F1-score = 0.7612, Accuracy = 0.7642\n",
      "SVM: F1-score = 0.7316, Accuracy = 0.7317\n",
      "KNN: F1-score = 0.6723, Accuracy = 0.6748\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fonction pour convertir les textes en vecteurs de mots\n",
    "def document_vector(model, doc):\n",
    "    return np.mean([model.wv[word] for word in doc if word in model.wv] or [np.zeros(embedding_dim)], axis=0)\n",
    "\n",
    "\n",
    "for technique, model in techniques.items():\n",
    "    if technique == \"Word Embeddings\":\n",
    "        X_train_tokens = [simple_preprocess(text) for text in X_train]\n",
    "        X_test_tokens = [simple_preprocess(text) for text in X_test]\n",
    "\n",
    "        # Entraînement du modèle Word2Vec\n",
    "        embedding_dim = 100\n",
    "        word2vec_model = Word2Vec(sentences=X_train_tokens, vector_size=embedding_dim, window=5, min_count=2, workers=4)\n",
    "\n",
    "        train_x = np.array([document_vector(word2vec_model, doc) for doc in X_train_tokens])\n",
    "        test_x = np.array([document_vector(word2vec_model, doc) for doc in X_test_tokens])\n",
    "\n",
    "\n",
    "    elif technique == \"NLP based features\":\n",
    "        continue\n",
    "\n",
    "    \n",
    "    elif technique == \"Prédicteurs de base sélectionnés\":\n",
    "        technique == \"Prédicteurs de base sélectionnés\"\n",
    "        print(50*\"=\")\n",
    "        print(f\"Technique: {technique}\")\n",
    "\n",
    "        X_base = X_rfe\n",
    "\n",
    "        print(f\"Shape:\",X_base.shape)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_base)\n",
    "        X = X_scaled\n",
    "        y = df['category_encoded']  # Les labels encodés\n",
    "        \n",
    "        train_x, test_x, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "    elif model is None:\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        print(50*\"=\")\n",
    "        print(f\"Technique: {technique}\")\n",
    "        train_x = model.fit_transform(X_train)\n",
    "        test_x = model.transform(X_test)\n",
    "\n",
    "    appliquer_model_classification(technique, train_x, test_x)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Accuracy</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Score F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modele</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVM</th>\n",
       "      <th>KNN</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Technique</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bag of Words</th>\n",
       "      <td>0.650407</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>0.869919</td>\n",
       "      <td>0.649696</td>\n",
       "      <td>0.975526</td>\n",
       "      <td>0.959421</td>\n",
       "      <td>0.872388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prédicteurs de base</th>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>0.869919</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.975719</td>\n",
       "      <td>0.983682</td>\n",
       "      <td>0.872388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prédicteurs de base sélectionnés</th>\n",
       "      <td>0.674797</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.672327</td>\n",
       "      <td>0.753115</td>\n",
       "      <td>0.761154</td>\n",
       "      <td>0.731589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TfidfVectorizer</th>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.935956</td>\n",
       "      <td>0.983752</td>\n",
       "      <td>0.983785</td>\n",
       "      <td>0.975833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word Embeddings</th>\n",
       "      <td>0.357724</td>\n",
       "      <td>0.373984</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.358414</td>\n",
       "      <td>0.340771</td>\n",
       "      <td>0.500697</td>\n",
       "      <td>0.106522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Accuracy                                    \\\n",
       "Modele                                 KNN Logistic Regression Random Forest   \n",
       "Technique                                                                      \n",
       "Bag of Words                      0.650407            0.975610      0.959350   \n",
       "Prédicteurs de base               0.682927            0.975610      0.983740   \n",
       "Prédicteurs de base sélectionnés  0.674797            0.756098      0.764228   \n",
       "TfidfVectorizer                   0.934959            0.983740      0.983740   \n",
       "Word Embeddings                   0.357724            0.373984      0.504065   \n",
       "\n",
       "                                            Score F1                      \\\n",
       "Modele                                 SVM       KNN Logistic Regression   \n",
       "Technique                                                                  \n",
       "Bag of Words                      0.869919  0.649696            0.975526   \n",
       "Prédicteurs de base               0.869919  0.681818            0.975719   \n",
       "Prédicteurs de base sélectionnés  0.731707  0.672327            0.753115   \n",
       "TfidfVectorizer                   0.975610  0.935956            0.983752   \n",
       "Word Embeddings                   0.243902  0.358414            0.340771   \n",
       "\n",
       "                                                          \n",
       "Modele                           Random Forest       SVM  \n",
       "Technique                                                 \n",
       "Bag of Words                          0.959421  0.872388  \n",
       "Prédicteurs de base                   0.983682  0.872388  \n",
       "Prédicteurs de base sélectionnés      0.761154  0.731589  \n",
       "TfidfVectorizer                       0.983785  0.975833  \n",
       "Word Embeddings                       0.500697  0.106522  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivot = df_metrics.pivot_table(index='Technique', columns='Modele', values=['Score F1', 'Accuracy'])\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 95.571648 secondes\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "    \n",
    "# Calcul du temps écoulé\n",
    "execution_time = end_time - start_time\n",
    "    \n",
    "print(f\"Temps d'exécution: {execution_time:.6f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
